# Chinese-BertWord-Embedding
## 利用Bert获取中文字、词向量

### 这个代码主要是本人需要一批预训练词向量，因此通过预训练模型进行转化；字、词向量质量不清楚，
### 本质上还是一个静态词向量，因此各位小伙伴还是不要疯狂的认为该词向量就好于word2vec,glove等词向量
### 词向量的本质上还是一堆用于字，词表示的符号，只是一个初始化的东西
### 本人在项目中一般都是随机初始化词向量的，随着神经网络模型的迭代，这些向量都会更新的(除非你将词向量的梯度锁死，不需要梯度)
### 词向量只是模型训练过程中的副产物，希望小伙伴有一个清楚的认识

## 获取不同维度的字、词向量
### 这里获取到的词向量就是768维的，想要其他维度的向量，可以加一个linear层进行映射(偷懒的方法) get_100dim_embed.py
### 同理，其他维度的只需修改这个文件中的linear层中的值
### 代码中的词向量部分没测试过，可以自行测试；字向量部分经过测试，是可以生成100维的向量的
